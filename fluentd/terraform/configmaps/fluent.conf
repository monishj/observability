@include kubernetes.conf
@include prometheus.conf
@include host.conf
@include systemd.conf

<system>
  log_level info
  <log>
    format json
  </log>
</system>

<match fluent.**>
  @type null
</match>

<match **>
  @type s3
  @id out_s3_local

  s3_bucket "#{ENV.fetch('BUCKET_NAME')}"
  s3_region "#{ENV.fetch('REGION')}"
  acl bucket-owner-full-control

  time_slice_format %Y%m%d_%H%M

  path cluster-logs/
  # Include the node name to avoid writing two pods creating the same s3 object and overwriting logs
  s3_object_key_format "%{path}%{time_slice}/cluster-log-%{index}-#{ENV['FLUENTD_NODE_NAME']}.%{file_extension}"

  # Inject additional key used by logstash
  <inject>
    time_key fluentd_time_ms
    time_type string
    time_format %Y-%m-%dT%H:%M:%S.%LZ
    tag_key tag
    localtime false
  </inject>

  <buffer>
    @type file
    path /var/log/fluent/s3_local
    # Time period in which fluentd gathers logs before sending it to s3
    timekey 60s
    timekey_wait 15s
    timekey_use_utc true
  </buffer>
  <format>
    @type json
  </format>

  <web_identity_credentials>
    role_arn                "#{ENV.fetch('ROLE_ARN')}"
    web_identity_token_file /var/run/secrets/eks.amazonaws.com/serviceaccount/token
    role_session_name       fluentd2
  </web_identity_credentials>
</match>
